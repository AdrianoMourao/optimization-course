{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847b6ce0-060c-4faa-beca-18892b198458",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "# One-vs-All Logistic Regression for handwritten digit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817c22e-1a70-4a7b-bae7-c7eaa8b9fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e1ea0-c997-4744-a290-c1a6e52d2c75",
   "metadata": {},
   "source": [
    "# Load and prepare data\n",
    "# Load data from ex3data1.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f5244-9c72-436d-842f-4b0937e12907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = loadmat('ex3data1.mat')\n",
    "X = data['X']  # Features (400-dimensional vectorized images)\n",
    "y = data['y'].flatten()  # Labels (1-10, with 10 representing '0')\n",
    "m = X.shape[0]  # Number of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfbbae-a39c-4b26-b3d0-ff42da37e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map label '10' to '0'\n",
    "y[y == 10] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058fcc4-a694-4374-aed8-4d7d1f8400df",
   "metadata": {},
   "source": [
    "# Visualize data\n",
    "# Function to display data points as a grid of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf408960-0be0-48dd-ae09-2d9a24aa295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayData(X, example_width=20):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    m, n = X.shape\n",
    "    example_height = n // example_width\n",
    "    display_rows = int(np.sqrt(m))\n",
    "    display_cols = m // display_rows\n",
    "    \n",
    "    pad = 1\n",
    "    display_array = -np.ones((pad + display_rows * (example_height + pad),\n",
    "                              pad + display_cols * (example_width + pad)))\n",
    "\n",
    "    for i in range(display_rows):\n",
    "        for j in range(display_cols):\n",
    "            if i * display_cols + j >= m:\n",
    "                break\n",
    "            example = X[i * display_cols + j, :].reshape(example_height, example_width)\n",
    "            display_array[pad + i * (example_height + pad):pad + i * (example_height + pad) + example_height,\n",
    "                          pad + j * (example_width + pad):pad + j * (example_width + pad) + example_width] = example / np.max(example)\n",
    "\n",
    "    plt.imshow(display_array.T, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display 100 random data points\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "displayData(sel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e0d18-42eb-4444-811c-2c845bf10ad6",
   "metadata": {},
   "source": [
    "# Sigmoid Function\n",
    "# Function to compute sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835f7d0-b0c1-4267-8db4-0e970066fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596c6df-5442-4fe6-bcda-41f811f81231",
   "metadata": {},
   "source": [
    "# Cost Function with Regularization\n",
    "# Compute cost and gradient for logistic regression with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba78369-9b9b-4e11-8747-953516f3e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrCostFunction(theta, X, y, lambda_):\n",
    "    m = len(y)\n",
    "    theta = theta[:, np.newaxis]\n",
    "    y = y[:, np.newaxis]\n",
    "    predictions = sigmoid(X @ theta)\n",
    "    cost = -(1 / m) * (y.T @ np.log(predictions) + (1 - y).T @ np.log(1 - predictions))\n",
    "    reg_term = (lambda_ / (2 * m)) * np.sum(theta[1:] ** 2)\n",
    "    cost = cost + reg_term\n",
    "\n",
    "    grad = (1 / m) * (X.T @ (predictions - y))\n",
    "    grad[1:] += (lambda_ / m) * theta[1:]\n",
    "    return cost[0, 0], grad.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab6cfb-c192-43ec-9058-aa4b9c305652",
   "metadata": {},
   "source": [
    "# One-vs-All Training\n",
    "# Train logistic regression classifiers for each digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136714a9-0f53-439e-a0fc-ebe195bce881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_labels, lambda_):\n",
    "    m, n = X.shape\n",
    "    all_theta = np.zeros((num_labels, n + 1))\n",
    "    X = np.column_stack((np.ones(m), X))\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        print(f\"Training classifier for digit {i}...\")\n",
    "        initial_theta = np.zeros(n + 1)\n",
    "        label = (y == i).astype(int)\n",
    "        result = minimize(lambda t: lrCostFunction(t, X, label, lambda_)[0],\n",
    "                          initial_theta,\n",
    "                          jac=lambda t: lrCostFunction(t, X, label, lambda_)[1],\n",
    "                          options={'maxiter': 50})\n",
    "        all_theta[i, :] = result.x\n",
    "    return all_theta\n",
    "\n",
    "lambda_ = 0.1\n",
    "num_labels = 10\n",
    "\n",
    "all_theta = oneVsAll(X, y, num_labels, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979106e2-93a8-4895-aa28-a8706b73976a",
   "metadata": {},
   "source": [
    "# Predict with One-vs-All Classifier\n",
    "# Predict the label for each example using trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2ebe5-8fa7-4821-a98a-287af74e6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_theta, X):\n",
    "    m = X.shape[0]\n",
    "    X = np.column_stack((np.ones(m), X))\n",
    "    predictions = X @ all_theta.T\n",
    "    return np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compute predictions and accuracy\n",
    "pred = predictOneVsAll(all_theta, X)\n",
    "accuracy = np.mean(pred == y) * 100\n",
    "\n",
    "print(f\"Training Set Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e31265-23f3-4f3e-83f6-83785edc1bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDataAndDecisionBoundary(X, y, theta):\n",
    "    # Plot data points\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    \n",
    "    plt.scatter(X[pos, 0], X[pos, 1], c='k', marker='+', label='Admitted')\n",
    "    plt.scatter(X[neg, 0], X[neg, 1], c='y', marker='o', label='Not admitted')\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    x_values = [np.min(X[:, 0]) - 2, np.max(X[:, 0]) + 2]\n",
    "    y_values = -(theta[0] + np.dot(theta[1], x_values)) / theta[2]\n",
    "    plt.plot(x_values, y_values, label='Decision Boundary', color='b')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Exam 1 score')\n",
    "    plt.ylabel('Exam 2 score')\n",
    "    plt.legend()\n",
    "    plt.title('Data and Decision Boundary')\n",
    "    plt.show()\n",
    "\n",
    "# Chamada da função com os dados e parâmetros\n",
    "plotDataAndDecisionBoundary(X[:, 1:], y, optimal_theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923c903-72ad-4062-b5c8-1f4360afb553",
   "metadata": {},
   "source": [
    "#  Predict and Compute Accuracy\n",
    "# Predict admission probabilities and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e8d47-dd3c-4b43-b583-b95925eb1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function\n",
    "def predict(theta, X):\n",
    "    return sigmoid(X @ theta) >= 0.5\n",
    "\n",
    "prob = sigmoid(np.array([1, 45, 85]) @ optimal_theta)\n",
    "print(f\"For a student with scores 45 and 85, we predict an admission probability of {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fb813-3b50-422a-9f3b-dfbb55319f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training set accuracy\n",
    "p = predict(optimal_theta, X)\n",
    "accuracy = np.mean(p == y) * 100\n",
    "print(f\"Train Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184f43b-bdf1-4e34-a40d-14eaefbd96b8",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "# Logistic Regression with Regularization for classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0afa95-7222-43ef-b258-4f210623aee5",
   "metadata": {},
   "source": [
    "# Load and prepare data\n",
    "# Load data from ex2data2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c6137-f3a2-4d1c-b01d-f1714c966910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.loadtxt('ex2data2.txt', delimiter=',')\n",
    "X = data[:, :2]  # Features\n",
    "Y = data[:, 2]   # Target: Labels (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2eedfc-42cb-4d70-aa94-1dfbdc1271c1",
   "metadata": {},
   "source": [
    "# Visualize data\n",
    "# Function to plot data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419d232-2180-42c6-a681-e0d1fa2ab591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y):\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    \n",
    "    plt.scatter(X[pos, 0], X[pos, 1], c='k', marker='+', label='y = 1')\n",
    "    plt.scatter(X[neg, 0], X[neg, 1], c='y', marker='o', label='y = 0')\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Plotting data...\")\n",
    "plotData(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf7a69e-e19e-4918-9f18-6237b816beb6",
   "metadata": {},
   "source": [
    "# Map Feature Function\n",
    "# Function to map features to polynomial terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d9a87-ca0d-419e-adf5-37b5d110f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(x1, x2):\n",
    "    degree = 6\n",
    "    out = np.ones(x1.shape[0])[:, np.newaxis]\n",
    "    for i in range(1, degree + 1):\n",
    "        for j in range(i + 1):\n",
    "            term = (x1 ** (i - j)) * (x2 ** j)\n",
    "            out = np.hstack((out, term[:, np.newaxis]))\n",
    "    return out\n",
    "\n",
    "# Map feature\n",
    "X = mapFeature(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db7967-2bc2-4051-9362-692d2b6008e8",
   "metadata": {},
   "source": [
    "# Sigmoid Function\n",
    "# Function to compute sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6bbce3-cde8-4231-940c-882b09a247b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b1ed7-796a-42d0-b793-8b6ab3fe1c7c",
   "metadata": {},
   "source": [
    "# Cost Function with Regularization\n",
    "# Compute cost and gradient for logistic regression with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086c942-34e1-4707-9cc4-64c1ae21ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function with regularization\n",
    "def costFunctionReg(theta, X, y, lambda_):\n",
    "    m = len(y)\n",
    "    predictions = sigmoid(X @ theta)\n",
    "    cost = -(1 / m) * (y @ np.log(predictions) + (1 - y) @ np.log(1 - predictions))\n",
    "    reg_term = (lambda_ / (2 * m)) * np.sum(theta[1:] ** 2)\n",
    "    cost += reg_term\n",
    "\n",
    "    grad = (1 / m) * (X.T @ (predictions - y))\n",
    "    grad[1:] += (lambda_ / m) * theta[1:]\n",
    "    return cost, grad\n",
    "\n",
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(X.shape[1])\n",
    "lambda_ = 1\n",
    "\n",
    "# Compute cost and gradient with initial theta\n",
    "cost, grad = costFunctionReg(initial_theta, X, Y, lambda_)\n",
    "print(f\"Cost at initial theta (zeros): {cost}\")\n",
    "print(f\"Gradient at initial theta (zeros): {grad[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee5aac-0a3e-47b7-aa64-88e731aeba52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7aa9f7-f8fb-4917-a14f-bcb953da3740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431103d0-5b23-44fe-b25e-0ed513a7a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface plot\n",
    "J_vals = J_vals.T  # Transpose for correct orientation\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "T0, T1 = np.meshgrid(theta0_vals, theta1_vals)\n",
    "ax.plot_surface(T0, T1, J_vals, cmap='viridis')\n",
    "ax.set_xlabel('theta_0')\n",
    "ax.set_ylabel('theta_1')\n",
    "ax.set_zlabel('Cost J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b23868-b262-44bb-97e0-836bc221c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour plot\n",
    "plt.contour(theta0_vals, theta1_vals, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')\n",
    "plt.xlabel('theta_0')\n",
    "plt.ylabel('theta_1')\n",
    "plt.plot(theta[0], theta[1], 'rx', markersize=10, linewidth=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6cb4f9-2e44-49f4-b812-5a3f712ba3f5",
   "metadata": {},
   "source": [
    "# Optimizing Theta\n",
    "# Use a built-in optimizer to find the optimal theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ac199-6763-4309-b3d3-c21096f0e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize using scipy.optimize.minimize\n",
    "result = minimize(lambda t: costFunctionReg(t, X, Y, lambda_)[0],\n",
    "                  initial_theta,\n",
    "                  jac=lambda t: costFunctionReg(t, X, Y, lambda_)[1],\n",
    "                  options={'maxiter': 400})\n",
    "\n",
    "optimal_theta = result.x\n",
    "cost_at_optimal_theta = result.fun\n",
    "\n",
    "print(f\"Cost at optimal theta: {cost_at_optimal_theta}\")\n",
    "print(f\"Optimal theta: {optimal_theta[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe2efb-e699-4423-bdd5-b8b169b31c28",
   "metadata": {},
   "source": [
    "# Plot Decision Boundary\n",
    "# Plot decision boundary using the optimal theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4865e-e32e-4679-8fb7-81e46c0ca113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDataAndDecisionBoundary(X, y, theta):\n",
    "    # Plot data points\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    \n",
    "    plt.scatter(X[pos, 0], X[pos, 1], c='k', marker='+', label='y = 1')\n",
    "    plt.scatter(X[neg, 0], X[neg, 1], c='y', marker='o', label='y = 0')\n",
    "    \n",
    "    # Create grid for contour plot\n",
    "    u = np.linspace(-1, 1.5, 50)\n",
    "    v = np.linspace(-1, 1.5, 50)\n",
    "    z = np.zeros((len(u), len(v)))\n",
    "\n",
    "    # Evaluate z = theta * features for grid points\n",
    "    for i in range(len(u)):\n",
    "        for j in range(len(v)):\n",
    "            z[i, j] = mapFeature(np.array([u[i]]), np.array([v[j]])).dot(theta).item()  # Use .item() to extract scalar\n",
    "\n",
    "    # Plot decision boundary\n",
    "    z = z.T  # Transpose for proper contour plotting\n",
    "    contour = plt.contour(u, v, z, levels=[0], linewidths=2, colors='g', linestyles='-')\n",
    "    \n",
    "    # Add a manual legend entry for the decision boundary\n",
    "    plt.plot([], [], 'g-', label='Decision Boundary')  # Dummy plot for legend\n",
    "    \n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.legend()\n",
    "    plt.title('Data and Decision Boundary')\n",
    "    plt.show()\n",
    "\n",
    "# Call the combined function with data and parameters\n",
    "print(\"Plotting data with decision boundary...\")\n",
    "plotDataAndDecisionBoundary(X[:, 1:3], Y, optimal_theta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508bc50-d0fb-41c3-9a3e-bf8461f38423",
   "metadata": {},
   "source": [
    "# Predict and Compute Accuracy\n",
    "# Predict probabilities and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c14339-3764-4062-ab6b-2c1a1eb0904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Predict function\n",
    "def predict(theta, X):\n",
    "    return sigmoid(X @ theta) >= 0.5\n",
    "\n",
    "p = predict(optimal_theta, X)\n",
    "accuracy = np.mean(p == Y) * 100\n",
    "print(f\"Train Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b359ca6-ecf2-49a7-ab94-f26b741c0739",
   "metadata": {},
   "source": [
    "### Training data with decision boundary $\\lambda$ = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ede2f6-6e4a-4ade-a554-b496ae893570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(X.shape[1])\n",
    "lambda_ = 0\n",
    "\n",
    "# Optimize using scipy.optimize.minimize\n",
    "result = minimize(lambda t: costFunctionReg(t, X, Y, lambda_)[0],\n",
    "                  initial_theta,\n",
    "                  jac=lambda t: costFunctionReg(t, X, Y, lambda_)[1],\n",
    "                  options={'maxiter': 400})\n",
    "\n",
    "optimal_theta = result.x\n",
    "cost_at_optimal_theta = result.fun\n",
    "\n",
    "print(f\"Cost at optimal theta: {cost_at_optimal_theta}\")\n",
    "print(f\"Optimal theta: {optimal_theta[:5]}\")\n",
    "\n",
    "def plotDataAndDecisionBoundary(X, y, theta):\n",
    "    # Plot data points\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    \n",
    "    plt.scatter(X[pos, 0], X[pos, 1], c='k', marker='+', label='y = 1')\n",
    "    plt.scatter(X[neg, 0], X[neg, 1], c='y', marker='o', label='y = 0')\n",
    "    \n",
    "    # Create grid for contour plot\n",
    "    u = np.linspace(-1, 1.5, 50)\n",
    "    v = np.linspace(-1, 1.5, 50)\n",
    "    z = np.zeros((len(u), len(v)))\n",
    "\n",
    "    # Evaluate z = theta * features for grid points\n",
    "    for i in range(len(u)):\n",
    "        for j in range(len(v)):\n",
    "            z[i, j] = mapFeature(np.array([u[i]]), np.array([v[j]])).dot(theta).item()  # Use .item() to extract scalar\n",
    "\n",
    "    # Plot decision boundary\n",
    "    z = z.T  # Transpose for proper contour plotting\n",
    "    contour = plt.contour(u, v, z, levels=[0], linewidths=2, colors='g', linestyles='-')\n",
    "    \n",
    "    # Add a manual legend entry for the decision boundary\n",
    "    plt.plot([], [], 'g-', label='Decision Boundary')  # Dummy plot for legend\n",
    "    \n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.legend()\n",
    "    plt.title(r'No regularization (Overfitting) ($\\lambda$ = 0)')\n",
    "    plt.show()\n",
    "\n",
    "# Call the combined function with data and parameters\n",
    "print(\"Plotting data with decision boundary...\")\n",
    "plotDataAndDecisionBoundary(X[:, 1:3], Y, optimal_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4056d5-0d5a-49ba-858a-c54cf635a3a6",
   "metadata": {},
   "source": [
    "### Training data with decision boundary $\\lambda$ = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273daebb-068a-494b-a4b6-c30b31f0d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(X.shape[1])\n",
    "lambda_ = 100\n",
    "\n",
    "# Optimize using scipy.optimize.minimize\n",
    "result = minimize(lambda t: costFunctionReg(t, X, Y, lambda_)[0],\n",
    "                  initial_theta,\n",
    "                  jac=lambda t: costFunctionReg(t, X, Y, lambda_)[1],\n",
    "                  options={'maxiter': 400})\n",
    "\n",
    "optimal_theta = result.x\n",
    "cost_at_optimal_theta = result.fun\n",
    "\n",
    "print(f\"Cost at optimal theta: {cost_at_optimal_theta}\")\n",
    "print(f\"Optimal theta: {optimal_theta[:5]}\")\n",
    "\n",
    "def plotDataAndDecisionBoundary(X, y, theta):\n",
    "    # Plot data points\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    \n",
    "    plt.scatter(X[pos, 0], X[pos, 1], c='k', marker='+', label='y = 1')\n",
    "    plt.scatter(X[neg, 0], X[neg, 1], c='y', marker='o', label='y = 0')\n",
    "    \n",
    "    # Create grid for contour plot\n",
    "    u = np.linspace(-1, 1.5, 50)\n",
    "    v = np.linspace(-1, 1.5, 50)\n",
    "    z = np.zeros((len(u), len(v)))\n",
    "\n",
    "    # Evaluate z = theta * features for grid points\n",
    "    for i in range(len(u)):\n",
    "        for j in range(len(v)):\n",
    "            z[i, j] = mapFeature(np.array([u[i]]), np.array([v[j]])).dot(theta).item()  # Use .item() to extract scalar\n",
    "\n",
    "    # Plot decision boundary\n",
    "    z = z.T  # Transpose for proper contour plotting\n",
    "    contour = plt.contour(u, v, z, levels=[0], linewidths=2, colors='g', linestyles='-')\n",
    "    \n",
    "    # Add a manual legend entry for the decision boundary\n",
    "    plt.plot([], [], 'g-', label='Decision Boundary')  # Dummy plot for legend\n",
    "    \n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.legend()\n",
    "    plt.title(r'Too much regularization (Underfitting) ($\\lambda$ = 100)')\n",
    "    plt.show()\n",
    "\n",
    "# Call the combined function with data and parameters\n",
    "print(\"Plotting data with decision boundary...\")\n",
    "plotDataAndDecisionBoundary(X[:, 1:3], Y, optimal_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06586ff4-beb5-4c4a-a940-e0da9ded7671",
   "metadata": {},
   "source": [
    "### Usando método do gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c719c01-8614-4cbd-ab33-767f0ef6580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for logistic regression with regularization\n",
    "def gradientDescentReg(X, y, theta, alpha, lambda_, num_iters):\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        cost, grad = costFunctionReg(theta, X, y, lambda_)\n",
    "        theta -= alpha * grad\n",
    "        J_history.append(cost)\n",
    "\n",
    "    return theta, J_history\n",
    "\n",
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(X.shape[1])\n",
    "lambda_ = 1\n",
    "alpha = 0.1\n",
    "num_iters = 5000\n",
    "\n",
    "# Perform gradient descent\n",
    "optimal_theta, J_history = gradientDescentReg(X, Y, initial_theta, alpha, lambda_, num_iters)\n",
    "cost_at_optimal_theta, _ = costFunctionReg(optimal_theta, X, Y, lambda_)\n",
    "\n",
    "print(f\"Cost at optimal theta: {cost_at_optimal_theta}\")\n",
    "print(f\"Optimal theta: {optimal_theta[:5]}\")\n",
    "\n",
    "# Plot cost history\n",
    "plt.plot(range(num_iters), J_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost History')\n",
    "plt.show()\n",
    "\n",
    "# Cost at optimal theta: 0.5290027422869218\n",
    "# Optimal theta: [ 1.27268739  0.62557016  1.1809665  -2.01919822 -0.91761468]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10aa5e3-5bfb-45d4-82ea-ab58f826d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call the combined function with data and parameters\n",
    "def plotDataAndDecisionBoundary(X, y, theta):\n",
    "    # Plot data points\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    \n",
    "    plt.scatter(X[pos, 0], X[pos, 1], c='k', marker='+', label='y = 1')\n",
    "    plt.scatter(X[neg, 0], X[neg, 1], c='y', marker='o', label='y = 0')\n",
    "    \n",
    "    # Create grid for contour plot\n",
    "    u = np.linspace(-1, 1.5, 50)\n",
    "    v = np.linspace(-1, 1.5, 50)\n",
    "    z = np.zeros((len(u), len(v)))\n",
    "\n",
    "    # Evaluate z = theta * features for grid points\n",
    "    for i in range(len(u)):\n",
    "        for j in range(len(v)):\n",
    "            z[i, j] = mapFeature(np.array([u[i]]), np.array([v[j]])).dot(theta).item()  # Use .item() to extract scalar\n",
    "\n",
    "    # Plot decision boundary\n",
    "    z = z.T  # Transpose for proper contour plotting\n",
    "    contour = plt.contour(u, v, z, levels=[0], linewidths=2, colors='g', linestyles='-')\n",
    "    \n",
    "    # Add a manual legend entry for the decision boundary\n",
    "    plt.plot([], [], 'g-', label='Decision Boundary')  # Dummy plot for legend\n",
    "    \n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.legend()\n",
    "    plt.title(r'Using Gradient Descent')\n",
    "    plt.show()\n",
    "print(\"Plotting data with decision boundary...\")\n",
    "plotDataAndDecisionBoundary(X[:, 1:3], Y, optimal_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d59170-7c93-412b-9330-899dbf84916d",
   "metadata": {},
   "source": [
    "### Um bom exercício é variar os hiperparâmetros lambda, niters, alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
