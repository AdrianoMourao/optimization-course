{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e37464",
   "metadata": {},
   "source": [
    "[M] Overview\n",
    "This notebook is a Python implementation of the MATLAB script `ex3.m`. It covers training and testing logistic regression classifiers for recognizing handwritten digits using a one-vs-all approach. We will use the following functions, implemented in Python:\n",
    "- `lrCostFunction`\n",
    "- `oneVsAll`\n",
    "- `predictOneVsAll`\n",
    "- `predict`\n",
    "- `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669218f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Y] Import Libraries\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4576cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Y] Load and Visualize Data\n",
    "# Load training data from MAT file\n",
    "data = loadmat('ex3data1.mat')\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "y[y == 10] = 0  # Map label '10' to '0'\n",
    "\n",
    "# Randomly select 100 data points to display\n",
    "m = X.shape[0]\n",
    "rand_indices = np.random.permutation(m)\n",
    "sel = X[rand_indices[:100], :]\n",
    "\n",
    "def displayData(X):\n",
    "    \"\"\"Displays 2D data in a nice grid.\"\"\"\n",
    "    example_width = int(np.round(np.sqrt(X.shape[1])))\n",
    "    example_height = int(X.shape[1] / example_width)\n",
    "\n",
    "    display_rows = int(np.sqrt(X.shape[0]))\n",
    "    display_cols = int(np.ceil(X.shape[0] / display_rows))\n",
    "    \n",
    "    pad = 1\n",
    "    display_array = - np.ones((pad + display_rows * (example_height + pad),\n",
    "                               pad + display_cols * (example_width + pad)))\n",
    "    curr_ex = 0\n",
    "    for j in range(display_rows):\n",
    "        for i in range(display_cols):\n",
    "            if curr_ex >= X.shape[0]:\n",
    "                break\n",
    "            max_val = np.max(np.abs(X[curr_ex, :]))\n",
    "            rows = pad + j * (example_height + pad) + np.arange(example_height)\n",
    "            cols = pad + i * (example_width + pad) + np.arange(example_width)\n",
    "            display_array[np.ix_(rows, cols)] = X[curr_ex, :].reshape(example_height, example_width) / max_val\n",
    "            curr_ex += 1\n",
    "        if curr_ex >= X.shape[0]:\n",
    "            break\n",
    "    plt.imshow(display_array, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "displayData(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e47e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Y] Test lrCostFunction\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def lrCostFunction(theta, X, y, lambda_):\n",
    "    m = y.size\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    J = (1 / m) * (-y.T.dot(np.log(h)) - (1 - y).T.dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(theta[1:]))\n",
    "    grad = (1 / m) * X.T.dot(h - y)\n",
    "    grad[1:] += (lambda_ / m) * theta[1:]\n",
    "    return J, grad\n",
    "\n",
    "theta_t = np.array([-2, -1, 1, 2])\n",
    "X_t = np.hstack([np.ones((5, 1)), np.arange(1, 16).reshape(5, 3) / 10])\n",
    "y_t = (np.array([1, 0, 1, 0, 1]) >= 0.5).astype(int)\n",
    "lambda_t = 3\n",
    "J, grad = lrCostFunction(theta_t, X_t, y_t, lambda_t)\n",
    "print('Cost:', J)\n",
    "print('Gradients:', grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e188a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Y] Train One-vs-All Logistic Regression\n",
    "def oneVsAll(X, y, num_labels, lambda_):\n",
    "    m, n = X.shape\n",
    "    all_theta = np.zeros((num_labels, n + 1))\n",
    "    X = np.hstack([np.ones((m, 1)), X])\n",
    "\n",
    "    for c in range(num_labels):\n",
    "        initial_theta = np.zeros(n + 1)\n",
    "        res = minimize(fun=lambda t: lrCostFunction(t, X, (y == c).astype(int), lambda_)[0],\n",
    "                      x0=initial_theta,\n",
    "                      jac=lambda t: lrCostFunction(t, X, (y == c).astype(int), lambda_)[1],\n",
    "                      method='TNC')\n",
    "        all_theta[c, :] = res.x\n",
    "    return all_theta\n",
    "\n",
    "lambda_ = 0.1\n",
    "all_theta = oneVsAll(X, y, 10, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Y] Predict Using One-vs-All Classifier\n",
    "def predictOneVsAll(all_theta, X):\n",
    "    m = X.shape[0]\n",
    "    X = np.hstack([np.ones((m, 1)), X])\n",
    "    predictions = X.dot(all_theta.T)\n",
    "    return np.argmax(predictions, axis=1)\n",
    "\n",
    "pred = predictOneVsAll(all_theta, X)\n",
    "accuracy = np.mean(pred == y) * 100\n",
    "print(f'Training Set Accuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
